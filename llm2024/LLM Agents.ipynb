{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "# I am hosting Llama 3.1 8B Instruct using VLLM for my class. You can do so\n",
    "# yourself, or use an OpenAI model such as gpt-4o-mini.\n",
    "import os\n",
    "BASE_URL = os.getenv(\"CS4973_BASE_URL\")\n",
    "API_KEY = os.getenv(\"CS4973_API_KEY\")\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "MODEL = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "CLIENT = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "\n",
    "def query_llama(messages):\n",
    "    \"\"\"\n",
    "    Sends a single user message to a chat model.\n",
    "    \"\"\"\n",
    "    resp = CLIENT.chat.completions.create(\n",
    "        messages = messages,\n",
    "        model = MODEL,\n",
    "        temperature=0, # Deterministic\n",
    "    )\n",
    "    print(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Do We Mean by LLM Agents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WebShop ([Yao et al, 2022](https://proceedings.neurips.cc/paper_files/paper/2022/hash/82ad13ec01f9fe44c01cb91814fd7b8c-Abstract-Conference.html)) is an older example and\n",
    "AgentBench ([Liu et al, 2024](https://openreview.net/forum?id=zAdUB0aCTQ) is more recent. There are many more benchmarks.\n",
    "\n",
    "An example from Tau-Bench ([Yao et al, 2024](https://arxiv.org/abs/2406.12045)):\n",
    "\n",
    "<img src=\"images/Tau Bench Example.png\" style=\"width: 50%;\">\n",
    "\n",
    "The LLM is interacting with both a database backend and the user. (i.e., outputs from the LLM are marked appropriately.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A \"Calculator\" Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we say LLMs that \"use tools\", we mean that we are using the LLM to produce\n",
    "some kind of structured output that we parse and use to take some action.\n",
    "\n",
    "An early example of tool use from [Cobbe et al,\n",
    "2021](https://arxiv.org/abs/2110.14168) uses Python to evaluate mathematical\n",
    "expressions, because LLMs are bad at arithmetic:\n",
    "\n",
    "<img src=\"images/GSM8K Calculator.png\" style=\"width: 50%;\">\n",
    "\n",
    "What is shown above is a hack to the decoding algorithm: we stop generating when\n",
    "we see the `=` token and use Python. We can take a different approach with a\n",
    "conversational model and instruct it to produce structured output.\n",
    "\n",
    "First, let's see what we get out of Llama 3.1 8B Instruct without a tool. \n",
    "(It is the wrong answer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The factorial of 48 (48!) is:\n",
      "\n",
      "1,679,616,000,000,000,000,000\n"
     ]
    }
   ],
   "source": [
    "query_llama([{\"role\": \"user\", \"content\": \"What is factorial of 48? Be concise.\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(It is the wrong answer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The factorial of a number is the product of all positive integers less than or equal to that number. \n",
      "\n",
      "Here's a Python function to calculate the factorial of a given number:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "def calculate_factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a given number.\n",
      "    \n",
      "    Args:\n",
      "        n (int): The number to calculate the factorial of.\n",
      "    \n",
      "    Returns:\n",
      "        int: The factorial of the given number.\n",
      "    \"\"\"\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
      "    elif n == 0 or n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return math.factorial(n)\n",
      "\n",
      "# Test the function\n",
      "n = 48\n",
      "result = calculate_factorial(n)\n",
      "print(f\"The factorial of {n} is: {result}\")\n",
      "```\n",
      "\n",
      "However, since the factorial of 48 is a very large number, it might exceed the maximum limit of an integer in Python. In that case, we can use the `math.prod` function to calculate the factorial:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "def calculate_factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a given number.\n",
      "    \n",
      "    Args:\n",
      "        n (int): The number to calculate the factorial of.\n",
      "    \n",
      "    Returns:\n",
      "        int: The factorial of the given number.\n",
      "    \"\"\"\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
      "    elif n == 0 or n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return math.prod(range(1, n + 1))\n",
      "\n",
      "# Test the function\n",
      "n = 48\n",
      "result = calculate_factorial(n)\n",
      "print(f\"The factorial of {n} is: {result}\")\n",
      "```\n",
      "\n",
      "This will give you the correct result, but keep in mind that it's a very large number.\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"You are a helpful assistant. If you're given a math problem, write Python code to solve.\"\n",
    "\n",
    "query_llama([\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"What is factorial of 48?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, this has to be done carefully. Not every problem requires calculation.\n",
    "The following response is a little ridiculous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Hancock was an American patriot, statesman, and Founding Father of the United States. He is best known for being the first signer of the United States Declaration of Independence in 1776. His signature on the document is large and bold, which has become an iconic symbol of American independence.\n",
      "\n",
      "Hancock was born in 1737 in Braintree, Massachusetts, and he went on to become a successful merchant and politician. He served as the president of the Continental Congress from 1775 to 1777 and again from 1785 to 1786. He also served as the governor of Massachusetts from 1780 to 1785.\n",
      "\n",
      "Hancock was known for his strong leadership and his commitment to American independence. He was a key figure in the American Revolution and played a significant role in shaping the country's early history.\n",
      "\n",
      "Interestingly, the phrase \"put your John Hancock on it\" originated from Hancock's signature on the Declaration of Independence. It is now commonly used to refer to signing one's name to a document.\n",
      "\n",
      "Here's a simple Python code snippet that simulates the signing of a document:\n",
      "\n",
      "```python\n",
      "def sign_document(name):\n",
      "    print(f\"Signing the document with {name}'s signature...\")\n",
      "\n",
      "sign_document(\"John Hancock\")\n",
      "```\n",
      "\n",
      "This code simply prints out a message indicating that the document is being signed with John Hancock's signature.\n"
     ]
    }
   ],
   "source": [
    "query_llama([\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"Who was John Hancock?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A \"Web Search\" Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we are building a chatbot interface to an airline's website. Under the\n",
    "hood, we may have functions such as the following available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS = \"\"\"\n",
    "You are a helpful assistant. Write Python code to solve math problems. To search the\n",
    "web you can use the following function:\n",
    "\n",
    "```\n",
    "web_search(query: str) -> str\n",
    "```\n",
    "\n",
    "This returns the text of the first Google search result for `query`.\n",
    "\"\"\"\n",
    "\n",
    "EXAMPLES = [\n",
    "    { \"role\": \"user\", \"content\": \"What is 10 * 83\"},\n",
    "    { \n",
    "        \"role\": \"assistant\", \n",
    "        \"content\": \"\"\"```python\n",
    "print(10 * 83)\n",
    "```\"\"\"\n",
    "    },\n",
    "    { \"role\": \"user\", \"content\": \"Who is the current president of the United States?\"},\n",
    "    { \n",
    "        \"role\": \"assistant\", \n",
    "        \"content\": \"\"\"```python\n",
    "print(web_search('current president of the United States'))\n",
    "```\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea is to prompt the model to use the functions (the example helps a lot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "weather = web_search(\"Boston weather today\")\n",
      "print(weather)\n",
      "```\n",
      "\n",
      "This will print the current weather in Boston. However, please note that this is a web search and the result may not be accurate or up-to-date.\n",
      "\n",
      "Alternatively, you can use a weather API to get the current temperature in Boston. \n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "def get_weather(city):\n",
      "    api_key = \"YOUR_API_KEY\"\n",
      "    base_url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}\"\n",
      "    response = requests.get(base_url)\n",
      "    data = response.json()\n",
      "    return data[\"main\"][\"temp\"] - 273.15  # Convert Kelvin to Celsius\n",
      "\n",
      "print(get_weather(\"Boston\"))\n",
      "```\n",
      "\n",
      "Replace \"YOUR_API_KEY\" with your actual OpenWeatherMap API key.\n"
     ]
    }
   ],
   "source": [
    "query_llama([\n",
    "    { \"role\": \"system\", \"content\": TOOLS},\n",
    "    *EXAMPLES,\n",
    "    { \"role\": \"user\", \"content\": \"How cold is it today in Boston?\" }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first result that I got:\n",
    "\n",
    "<img src=\"images/Boston Weather.png\" style=\"width: 50%;\">\n",
    "\n",
    "I can copy-paste some text from it. (It should be easy to automate.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the output, it is 49°F (49 degrees Fahrenheit) in Boston today.\n"
     ]
    }
   ],
   "source": [
    "query_llama([\n",
    "    { \n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"I ran the following code:\n",
    "\n",
    "```python\n",
    "weather = web_search(\"Boston weather today\")\n",
    "print(weather)\n",
    "```\n",
    "\n",
    "The code output the following:\n",
    "\n",
    "```\n",
    "49° 94% SSE 14mph\n",
    "\n",
    "Rain showers early will evolve into a more steady rain overnight. Low 49F. Winds SSE at 10 to 20 mph. Chance of rain 90%. Rainfall around a quarter of an inch.\n",
    "\n",
    "Humidity 70%    UV Index 0 of 11\n",
    "Moonrise 4:21 am     Moonset 4:25 pm\n",
    "Waning Crescent\n",
    "```\n",
    "\n",
    "Now respond to this: How cold is it today in Boston?\n",
    "\"\"\"\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a little lame it it refers to the \"output\". But, making it sound more natural is a prompting exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Travel Agent Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a homework assigment in a class I taught:\n",
    "\n",
    "https://pages.github.khoury.northeastern.edu/arjunguha/engineering-llm-systems/hw/thomas_the_travel_agent/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing State, Memory, and Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs are stateless. Every stateful conversation involves sending in progressing longer input. The *context length* is the maximum number of input + output tokens the LLM can handle. For GPT-4o, it is 128,000 tokens total. A token is approximately 3.1 English characters. So, in kilobytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387.5"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(128_000 * 3.1) / 1_024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models are being trained to handle millions of tokens. But, it doesn't seem to be going very well.\n",
    "\n",
    "<img src=\"images/Same Task, More Tokens.png\" style=\"width:75%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in practical terms you need to manage context in some way, even if the model\n",
    "hypothetically supports very long context. Simplest approach is to \"forget\"\n",
    "earlier parts of the conversation. This may not work with most agent designs,\n",
    "e.g., if the problem statement is the first message, after a while the model\n",
    "won't see the problem statement. \n",
    "\n",
    "There is an approach discussed here that is used in practice:\n",
    "\n",
    "https://lexfridman.com/cursor-team-transcript#chapter8_prompt_engineering\n",
    "\n",
    "Other approaches involve applying information retrieval techniques to the context\n",
    "itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Self-Reflection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Shinn et al, 2023](https://proceedings.neurips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html) shows that sufficiently capable models can correct errors in their own reasoning *given some external signal.*  E.g., if you ask the model to write code and then run it. If execution produces an error, the model may be able to generate an alternative solution conditioned on the error message. This allows agents to make progress when they would otherwise get stuck.\n",
    "\n",
    "Here is an example from ChatGPT: https://chatgpt.com/share/672156e1-8a78-800d-9599-e135411dffb5\n",
    "\n",
    "\n",
    "*User:* I've attached a JSONL file. Write code to turn it into latex formatted as follows. For each item:\n",
    "\n",
    "```\n",
    "\\textbf{Problem:} \\lstinline|item[\"problem_name\"]|\n",
    "\\textbf{Signature:} \\lstinline|item[\"signature\"]|\n",
    "\\textbf{Clues:} \n",
    "\\begin{enumerate}\n",
    "\\item item[\"tags:][0]\n",
    "...\n",
    "\\end{enumerate}\n",
    "```\n",
    "\n",
    "*Assistant*: \n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "# Load the JSONL file\n",
    "file_path = '/mnt/data/problem_clues.jsonl'\n",
    "\n",
    "# Read the JSONL file and parse it\n",
    "data = []\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Create the LaTeX formatted string based on the provided format\n",
    "latex_output = \"\"\n",
    "for item in data:\n",
    "    latex_output += \"\\\\textbf{Problem:} \\\\lstinline|{}|\\n\".format(item.get(\"problem_name\", \"\"))\n",
    "    latex_output += \"\\\\textbf{Signature:} \\\\lstinline|{}|\\n\".format(item.get(\"signature\", \"\"))\n",
    "    latex_output += \"\\\\textbf{Clues:}\\n\\\\begin{enumerate}\\n\"\n",
    "    for tag in item.get(\"tags\", []):\n",
    "        latex_output += \"\\\\item {}\\n\".format(tag)\n",
    "    latex_output += \"\\\\end{enumerate}\\n\\n\"\n",
    "\n",
    "# Display the result\n",
    "latex_output\n",
    "```\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "Cell In[1], line 15\n",
    "     13 latex_output = \"\"\n",
    "     14 for item in data:\n",
    "---> 15     latex_output += \"\\\\textbf{Problem:} \\\\lstinline|{}|\\n\".format(item.get(\"problem_name\", \"\"))\n",
    "     16     latex_output += \"\\\\textbf{Signature:} \\\\lstinline|{}|\\n\".format(item.get(\"signature\", \"\"))\n",
    "     17     latex_output += \"\\\\textbf{Clues:}\\n\\\\begin{enumerate}\\n\"\n",
    "\n",
    "KeyError: 'Problem'\n",
    "```\n",
    "\n",
    "```python\n",
    "# Let's first inspect the structure of the JSON data to understand the issue.\n",
    "data[:2]  # Inspect first two items for structure\n",
    "```\n",
    "\n",
    "```\n",
    "[{'signature': 'def add_int(lst, num):',\n",
    "  'tags': ['edge case of list in list',\n",
    "   'concatenate num to strings',\n",
    "   'add num to integers',\n",
    "   'return list'],\n",
    "  'tests': [{'input': \"['hello', 'aaa', 'bbb', 'ccc', 1, 4], 5\",\n",
    "    'output': \"['hello5', 'aaa5', 'bbb5', 'ccc5', 6, 9]\"},\n",
    "   {'input': \"[8, 56, 8, '5', '4'], 1\", 'output': \"[9, 57, 9, '51', '41']\"},\n",
    "   {'input': \"['hello', 'aaa', 'bbb', ['list'], 1, 4], 5\",\n",
    "    'output': \"['hello5', 'aaa5', 'bbb5', ['list'], 6, 9]\"}],\n",
    "  'working_description': 'Given a list lst of strings and integers and integer num, concatenates num to the end of strings in lst and adds it to integers in lst.',\n",
    "  'problem_name': 'add_int'},\n",
    " {'signature': 'def add_up(arr):',\n",
    "  'tags': ['2D array',\n",
    "   'sum integer',\n",
    "   'sum float',\n",
    "   'return the sum of all elements',\n",
    "   'mention 0 base case',\n",
    "   'misdirection - add number within string'],\n",
    "  'tests': [{'input': \"[ [ 'one', 2, 3, 4 ] ]\", 'output': '9'},\n",
    "   {'input': \"[ [ 3, 'hello', 3.4 ] ]\", 'output': '6.4'},\n",
    "   {'input': '[ [ ] ]', 'output': '0'}],\n",
    "  'working_description': 'Add up all the elements in 2D array only if they are integer or float',\n",
    "  'problem_name': 'add_up'}]\n",
    "  ```\n",
    "\n",
    "  ...\n",
    "\n",
    "  The error message might be misleading. Since the dictionary key is confirmed to be correct, the issue may lie in how I am formatting or processing the data. I will try an alternative approach to debug it further.\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "```python\n",
    "# Clean up the formatting process and add more clarity\n",
    "latex_output = \"\"\n",
    "for idx, item in enumerate(data):\n",
    "    # Use get to avoid KeyErrors\n",
    "    problem_name = item.get(\"problem_name\", f\"Problem_{idx}\")\n",
    "    signature = item.get(\"signature\", \"Unknown Signature\")\n",
    "    tags = item.get(\"tags\", [])\n",
    "...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
